{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to run audio-visual speech separation (AVSS) inference using our pretrained RTFS-Net model.\n",
        "\n",
        "Project code and description: https://github.com/avsshw/AVSS"
      ],
      "metadata": {
        "id": "70X1Nnldhzab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "4CQfDLrBbEiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, clone the repository:"
      ],
      "metadata": {
        "id": "DJnoCawJiBiO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BWaErhOU1Vuw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22888ceb-f8e7-4a87-8bc6-316d89f10d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AVSS'...\n",
            "remote: Enumerating objects: 414, done.\u001b[K\n",
            "remote: Counting objects: 100% (414/414), done.\u001b[K\n",
            "remote: Compressing objects: 100% (261/261), done.\u001b[K\n",
            "remote: Total 414 (delta 212), reused 320 (delta 142), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (414/414), 1.51 MiB | 29.81 MiB/s, done.\n",
            "Resolving deltas: 100% (212/212), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/avsshw/AVSS.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AVSS"
      ],
      "metadata": {
        "id": "Ua0RqhLO1grP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7554569-257c-4d73-eaa1-9361b2cde141"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AVSS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "7_PSL6Rc1it3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We trained our model on the dataset of the following format:\n",
        "\n",
        "\n",
        "```bash\n",
        "NameOfTheDirectoryWithUtterances/\n",
        "├── audio/\n",
        "│   ├── train/\n",
        "│   │   ├── mix/          # Mixed audio utterances (2-speaker mixtures)\n",
        "│   │   ├── s1/          \n",
        "│   │   └── s2/          \n",
        "│   └── val/\n",
        "│       ├── mix/\n",
        "│       ├── s1/\n",
        "│       └── s2/\n",
        "└── mouths/\n",
        "    ├── train/            \n",
        "    │   ├── SpeakerID1.npz\n",
        "    │   ├── SpeakerID2.npz\n",
        "    │   └── ...\n",
        "    └── val/\n",
        "        ├── SpeakerID1.npz\n",
        "        ├── SpeakerID2.npz\n",
        "        └── ...\n",
        "```"
      ],
      "metadata": {
        "id": "kmFKLUGXXZx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be evaluated with the following commands which you can follow if needed:\n",
        "\n",
        "1.\n",
        "```bash\n",
        "gdown https://drive.google.com/uc?id=1t7FFsG3hPcgUYuitekSMpggYLvzV6SXW\n",
        "unzip /content/AVSS-main/rtfs.zip\n",
        "```\n",
        "\n",
        "2. Assuming you have dla_dataset directory in the root of the project (or you can change it via hydra option datasets.test.data_dir=your/dataset):\n",
        "```bash\n",
        "python3 inference.py inferencer.from_pretrained=rtfs_improved/model_best.pth datasets=val_inference\n",
        "```"
      ],
      "metadata": {
        "id": "7w_1MXgGabBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "!uv run python3 inference.py inferencer.from_pretrained=rtfs_improved/model_best.pth datasets=val_inference"
      ],
      "metadata": {
        "id": "-R6kcFpxxWay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom inference"
      ],
      "metadata": {
        "id": "S4UTBE0rbr1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wish to run inference on your custom dataset, our model expects data in the following structure:\n",
        "\n",
        "\n",
        "```bash\n",
        "NameOfTheDirectoryWithUtterances\n",
        "├── audio\n",
        "│   ├── mix\n",
        "│   │   ├── FirstSpeakerID1_SecondSpeakerID1.wav # also may be flac or mp3\n",
        "│   │   ├── FirstSpeakerID2_SecondSpeakerID2.wav\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   └── FirstSpeakerIDn_SecondSpeakerIDn.wav\n",
        "│   ├── s1 # ground truth for the speaker s1, may not be given\n",
        "│   │   ├── FirstSpeakerID1_SecondSpeakerID1.wav # also may be flac or mp3\n",
        "│   │   ├── FirstSpeakerID2_SecondSpeakerID2.wav\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   └── FirstSpeakerIDn_SecondSpeakerIDn.wav\n",
        "│   └── s2 # ground truth for the speaker s2, may not be given\n",
        "│       ├── FirstSpeakerID1_SecondSpeakerID1.wav # also may be flac or mp3\n",
        "│       ├── FirstSpeakerID2_SecondSpeakerID2.wav\n",
        "│       .\n",
        "│       .\n",
        "│       .\n",
        "│       └── FirstSpeakerIDn_SecondSpeakerIDn.wav\n",
        "└── mouths # contains video information for all speakers\n",
        "    ├── FirstOrSecondSpeakerID1.npz # npz mouth-crop\n",
        "    ├── FirstOrSecondSpeakerID2.npz\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "    └── FirstOrSecondSpeakerIDn.npz\n",
        "```"
      ],
      "metadata": {
        "id": "WtmD-ubJ13ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We provide a small example dataset with ground truths. To run inference:**"
      ],
      "metadata": {
        "id": "WnBlDTEcbIuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Download the data (you can pass the link to your YandexDisk dataset in the .zip format here):"
      ],
      "metadata": {
        "id": "FtauQ46BbMmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv run python3 scripts/download_inference_data.py --link https://disk.yandex.ru/d/h2t8ItWMdne2ZA --download_location ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwePmLjI3KFV",
        "outputId": "cf2f2ad4-93aa-4ec4-870d-2d394ac01d87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/scripts/download_inference_data.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our example dataset you can also use\n",
        "```bash\n",
        "sh scripts/inference.sh\n",
        "```\n",
        "\n",
        "but we provide you with a full comand for your own use above."
      ],
      "metadata": {
        "id": "KfCgo9pMj3FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Download the pretrained model:"
      ],
      "metadata": {
        "id": "Z4IhED4Piooi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1l72LuBr_CQxaut6WUbyyPFJIRIbH8-68\n",
        "!unzip /content/AVSS/rtfs.zip"
      ],
      "metadata": {
        "id": "JtXExN3c4T2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1008f56b-1ae4-415d-b12f-d3bd11d2018e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1l72LuBr_CQxaut6WUbyyPFJIRIbH8-68\n",
            "From (redirected): https://drive.google.com/uc?id=1l72LuBr_CQxaut6WUbyyPFJIRIbH8-68&confirm=t&uuid=e53e687e-9af1-419e-b673-8e9b7727b2ea\n",
            "To: /content/AVSS/rtfs.zip\n",
            "100% 311M/311M [00:01<00:00, 259MB/s]\n",
            "Archive:  /content/AVSS/rtfs.zip\n",
            "   creating: rtfs_improved/\n",
            "  inflating: rtfs_improved/checkpoint-epoch80.pth  \n",
            "  inflating: rtfs_improved/config.yaml  \n",
            "  inflating: rtfs_improved/checkpoint-epoch110.pth  \n",
            "  inflating: rtfs_improved/info.log  \n",
            "  inflating: rtfs_improved/checkpoint-epoch20.pth  \n",
            "  inflating: rtfs_improved/checkpoint-epoch60.pth  \n",
            "  inflating: rtfs_improved/checkpoint-epoch120.pth  \n",
            "  inflating: rtfs_improved/checkpoint-epoch190.pth  \n",
            "  inflating: rtfs_improved/checkpoint-epoch150.pth  \n",
            "  inflating: rtfs_improved/git_diff.patch  \n",
            "  inflating: rtfs_improved/checkpoint-epoch170.pth  \n",
            "  inflating: rtfs_improved/checkpoint-epoch10.pth  \n",
            "  inflating: rtfs_improved/checkpoint-epoch200.pth  \n",
            "  inflating: rtfs_improved/checkpoint-epoch30.pth  \n",
            "  inflating: rtfs_improved/checkpoint-epoch90.pth  \n",
            " extracting: rtfs_improved/git_commit.txt  \n",
            "  inflating: rtfs_improved/checkpoint-epoch160.pth  \n",
            "  inflating: rtfs_improved/checkpoint-epoch130.pth  \n",
            "  inflating: rtfs_improved/checkpoint-epoch140.pth  \n",
            "  inflating: rtfs_improved/model_best.pth  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Set environment & run inference:\n",
        "\n",
        "(Predictions will be saved to data/saved/inference_custom_dir/test, works even if you don't have ground truth like we do)"
      ],
      "metadata": {
        "id": "-vmWUYpqitMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "!sh scripts/inference.sh"
      ],
      "metadata": {
        "id": "lSyxCZgA118h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8aa777-bd53-4b26-fb7c-fb18a377aa65"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPython \u001b[36m3.12.11\u001b[39m\u001b[36m\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m113 packages\u001b[0m \u001b[2min 706ms\u001b[0m\u001b[0m\n",
            "RTFSNet(\n",
            "  (encoder): STFT()\n",
            "  (decoder): ISTFT()\n",
            "  (blocks): ModuleList(\n",
            "    (0): RTFSBlock(\n",
            "      (input_proj): Linear(in_features=2, out_features=64, bias=True)\n",
            "      (freq_rnn): FrequencyRNN(\n",
            "        (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (time_rnn): TimeRNN(\n",
            "        (rnn): LSTM(128, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (tf_interaction): TFInteraction(\n",
            "        (freq_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (time_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "        )\n",
            "        (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (output_proj): Linear(in_features=128, out_features=64, bias=True)\n",
            "    )\n",
            "    (1-3): 3 x RTFSBlock(\n",
            "      (input_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (freq_rnn): FrequencyRNN(\n",
            "        (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (time_rnn): TimeRNN(\n",
            "        (rnn): LSTM(128, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (tf_interaction): TFInteraction(\n",
            "        (freq_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (time_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "        )\n",
            "        (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (output_proj): Linear(in_features=128, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (mask_estimator): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=4, bias=True)\n",
            "    (3): Tanh()\n",
            "  )\n",
            ")\n",
            "Loading model weights from: rtfs_improved/model_best.pth ...\n",
            "test: 100% 3/3 [00:03<00:00,  1.16s/it]\n",
            "    test_SI-SNRi   : 12.02201239267985\n",
            "    test_SDRi      : 12.440301577250162\n",
            "    test_PESQ      : 2.0738654931386313\n",
            "    test_STOI      : 0.8945748607317606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your dataset folder is named something other than \"inference_dataset\",\n",
        "run full version of command where you can specify your dataset name.\n",
        "\n",
        "```bash\n",
        "import os\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "!uv run python3 inference.py \\\n",
        "    datasets.test.data_dir=YOUR_DATASET_NAME \\\n",
        "    inferencer.from_pretrained=rtfs_improved/model_best.pth \\\n",
        "    datasets=custom_dir\n",
        "```"
      ],
      "metadata": {
        "id": "x0AheTOba_-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "!uv run python3 inference.py \\\n",
        "    datasets.test.data_dir=inference_dataset \\\n",
        "    inferencer.from_pretrained=rtfs_improved/model_best.pth \\\n",
        "    datasets=custom_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzyU3DR0D_-M",
        "outputId": "5e764c86-9e52-4691-d130-0d1fde20dc83"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RTFSNet(\n",
            "  (encoder): STFT()\n",
            "  (decoder): ISTFT()\n",
            "  (blocks): ModuleList(\n",
            "    (0): RTFSBlock(\n",
            "      (input_proj): Linear(in_features=2, out_features=64, bias=True)\n",
            "      (freq_rnn): FrequencyRNN(\n",
            "        (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (time_rnn): TimeRNN(\n",
            "        (rnn): LSTM(128, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (tf_interaction): TFInteraction(\n",
            "        (freq_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (time_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "        )\n",
            "        (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (output_proj): Linear(in_features=128, out_features=64, bias=True)\n",
            "    )\n",
            "    (1-3): 3 x RTFSBlock(\n",
            "      (input_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (freq_rnn): FrequencyRNN(\n",
            "        (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (time_rnn): TimeRNN(\n",
            "        (rnn): LSTM(128, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (tf_interaction): TFInteraction(\n",
            "        (freq_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (time_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "        )\n",
            "        (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (output_proj): Linear(in_features=128, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (mask_estimator): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=4, bias=True)\n",
            "    (3): Tanh()\n",
            "  )\n",
            ")\n",
            "Loading model weights from: rtfs_improved/model_best.pth ...\n",
            "test: 100% 3/3 [00:02<00:00,  1.26it/s]\n",
            "    test_SI-SNRi   : 12.02201239267985\n",
            "    test_SDRi      : 12.440301577250162\n",
            "    test_PESQ      : 2.0738654931386313\n",
            "    test_STOI      : 0.8945748607317606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluate metrics (Optional)"
      ],
      "metadata": {
        "id": "l84ZXI5_cPbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your dataset includes ground-truth clean sources (s1/, s2/), you can compute metrics separately:"
      ],
      "metadata": {
        "id": "4nU0LUY8gPFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "!uv run python3 calc_metrics.py \\\n",
        "--predictions_dir data/saved/inference_custom_dir/test \\\n",
        "--ground_truth_dir inference_dataset/audio \\\n",
        "--mixture_dir inference_dataset/audio/mix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kebOdzE-f38K",
        "outputId": "08dbf172-817a-468b-9575-5289961f9a45"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 3/3 [00:01<00:00,  1.54it/s]\n",
            "SI-SNRi        : 12.0220\n",
            "SDRi           : 12.4403\n",
            "PESQ           : 2.0737\n",
            "STOI           : 0.8946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script assumes:\n",
        "\n",
        "Predictions are .wav files with the same names as mixtures.\n",
        "\n",
        "(by default they are saved in data/saved/inference_custom_dir/test)\n",
        "\n",
        "Ground truth is split into s1/ and s2/ subdirectories.\n",
        "\n",
        "Mixtures are in mixture_dir."
      ],
      "metadata": {
        "id": "9ExptSgTi_Bf"
      }
    }
  ]
}