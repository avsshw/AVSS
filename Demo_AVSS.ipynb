{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to run audio-visual speech separation (AVSS) inference using our pretrained RTFS-Net model.\n",
        "\n",
        "Project code and description: https://github.com/avsshw/AVSS"
      ],
      "metadata": {
        "id": "70X1Nnldhzab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "4CQfDLrBbEiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, clone the repository:"
      ],
      "metadata": {
        "id": "DJnoCawJiBiO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWaErhOU1Vuw"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/avsshw/AVSS.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AVSS"
      ],
      "metadata": {
        "id": "Ua0RqhLO1grP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "7_PSL6Rc1it3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We trained our model on the dataset of the following format:\n",
        "\n",
        "\n",
        "```bash\n",
        "NameOfTheDirectoryWithUtterances/\n",
        "├── audio/\n",
        "│   ├── train/\n",
        "│   │   ├── mix/          # Mixed audio utterances (2-speaker mixtures)\n",
        "│   │   ├── s1/          \n",
        "│   │   └── s2/          \n",
        "│   └── val/\n",
        "│       ├── mix/\n",
        "│       ├── s1/\n",
        "│       └── s2/\n",
        "└── mouths/\n",
        "    ├── train/            \n",
        "    │   ├── SpeakerID1.npz\n",
        "    │   ├── SpeakerID2.npz\n",
        "    │   └── ...\n",
        "    └── val/\n",
        "        ├── SpeakerID1.npz\n",
        "        ├── SpeakerID2.npz\n",
        "        └── ...\n",
        "```"
      ],
      "metadata": {
        "id": "kmFKLUGXXZx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be evaluated with the following commands which you can follow if needed:\n",
        "\n",
        "1.\n",
        "```bash\n",
        "gdown https://drive.google.com/uc?id=1t7FFsG3hPcgUYuitekSMpggYLvzV6SXW\n",
        "unzip /content/AVSS-main/rtfs.zip\n",
        "```\n",
        "\n",
        "2. Assuming you have dla_dataset directory in the root of the project (or you can change it via hydra option datasets.test.data_dir=your/dataset):\n",
        "```bash\n",
        "python3 inference.py inferencer.from_pretrained=rtfs_improved/model_best.pth datasets=val_inference\n",
        "```"
      ],
      "metadata": {
        "id": "7w_1MXgGabBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "!uv run python3 inference.py inferencer.from_pretrained=rtfs_improved/model_best.pth datasets=val_inference"
      ],
      "metadata": {
        "id": "-R6kcFpxxWay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom inference"
      ],
      "metadata": {
        "id": "S4UTBE0rbr1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wish to run inference on your custom dataset, our model expects data in the following structure:\n",
        "\n",
        "\n",
        "```bash\n",
        "NameOfTheDirectoryWithUtterances\n",
        "├── audio\n",
        "│   ├── mix\n",
        "│   │   ├── FirstSpeakerID1_SecondSpeakerID1.wav # also may be flac or mp3\n",
        "│   │   ├── FirstSpeakerID2_SecondSpeakerID2.wav\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   └── FirstSpeakerIDn_SecondSpeakerIDn.wav\n",
        "│   ├── s1 # ground truth for the speaker s1, may not be given\n",
        "│   │   ├── FirstSpeakerID1_SecondSpeakerID1.wav # also may be flac or mp3\n",
        "│   │   ├── FirstSpeakerID2_SecondSpeakerID2.wav\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   └── FirstSpeakerIDn_SecondSpeakerIDn.wav\n",
        "│   └── s2 # ground truth for the speaker s2, may not be given\n",
        "│       ├── FirstSpeakerID1_SecondSpeakerID1.wav # also may be flac or mp3\n",
        "│       ├── FirstSpeakerID2_SecondSpeakerID2.wav\n",
        "│       .\n",
        "│       .\n",
        "│       .\n",
        "│       └── FirstSpeakerIDn_SecondSpeakerIDn.wav\n",
        "└── mouths # contains video information for all speakers\n",
        "    ├── FirstOrSecondSpeakerID1.npz # npz mouth-crop\n",
        "    ├── FirstOrSecondSpeakerID2.npz\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "    └── FirstOrSecondSpeakerIDn.npz\n",
        "```"
      ],
      "metadata": {
        "id": "WtmD-ubJ13ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We provide a small example dataset with ground truths. To run inference:**"
      ],
      "metadata": {
        "id": "WnBlDTEcbIuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Download the data (you can pass the link to your YandexDisk dataset in the .zip format here):"
      ],
      "metadata": {
        "id": "FtauQ46BbMmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv run python3 scripts/download_inference_data.py --link https://disk.yandex.ru/d/h2t8ItWMdne2ZA --download_location ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwePmLjI3KFV",
        "outputId": "ff434c96-3967-4d99-c15c-532de2761b90"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPython \u001b[36m3.12.11\u001b[39m\u001b[36m\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m113 packages\u001b[0m \u001b[2min 654ms\u001b[0m\u001b[0m\n",
            "Download complete.\n",
            "Extracting ./inference_dataset.zip...\n",
            "Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our example dataset you can also use\n",
        "```bash\n",
        "sh scripts/inference.sh\n",
        "```\n",
        "\n",
        "but we provide you with a full comand for your own use above."
      ],
      "metadata": {
        "id": "KfCgo9pMj3FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Download the pretrained model:"
      ],
      "metadata": {
        "id": "Z4IhED4Piooi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1l72LuBr_CQxaut6WUbyyPFJIRIbH8-68\n",
        "!unzip /content/AVSS/rtfs.zip"
      ],
      "metadata": {
        "id": "JtXExN3c4T2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Set environment & run inference:\n",
        "\n",
        "(Predictions will be saved to data/saved/inference_custom_dir/test, works even if you don't have ground truth like we do)"
      ],
      "metadata": {
        "id": "-vmWUYpqitMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "!sh scripts/inference.sh"
      ],
      "metadata": {
        "id": "lSyxCZgA118h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your dataset folder is named something other than \"inference_dataset\",\n",
        "replace !sh scripts/inference.sh command with the command below:\n",
        "```bash\n",
        "!uv run python3 inference.py \\\n",
        "    datasets.test.data_dir=YOUR_DATASET_NAME \\\n",
        "    inferencer.from_pretrained=rtfs_improved/model_best.pth \\\n",
        "    datasets=custom_dir\n",
        "```"
      ],
      "metadata": {
        "id": "x0AheTOba_-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluate metrics (Optional)"
      ],
      "metadata": {
        "id": "l84ZXI5_cPbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your dataset includes ground-truth clean sources (s1/, s2/), you can compute metrics separately:"
      ],
      "metadata": {
        "id": "4nU0LUY8gPFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv run python3 calc_metrics.py \\\n",
        "--predictions_dir data/saved/inference_custom_dir/test \\\n",
        "--ground_truth_dir inference_dataset/audio \\\n",
        "--mixture_dir inference_dataset/audio/mix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kebOdzE-f38K",
        "outputId": "8c4983b3-311e-4bfd-cab8-f3aaefb86a39"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 3/3 [00:02<00:00,  1.46it/s]\n",
            "SI-SNRi        : 12.0220\n",
            "SDRi           : 12.4403\n",
            "PESQ           : 2.0737\n",
            "STOI           : 0.8946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script assumes:\n",
        "\n",
        "Predictions are .wav files with the same names as mixtures.\n",
        "\n",
        "(by default they are saved in data/saved/inference_custom_dir/test)\n",
        "\n",
        "Ground truth is split into s1/ and s2/ subdirectories.\n",
        "\n",
        "Mixtures are in mixture_dir."
      ],
      "metadata": {
        "id": "9ExptSgTi_Bf"
      }
    }
  ]
}
