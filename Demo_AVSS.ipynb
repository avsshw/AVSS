{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to run audio-visual speech separation (AVSS) inference using our pretrained RTFS-Net model.\n",
        "\n",
        "Project code and description: https://github.com/avsshw/AVSS"
      ],
      "metadata": {
        "id": "70X1Nnldhzab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "4CQfDLrBbEiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, clone the repository:"
      ],
      "metadata": {
        "id": "DJnoCawJiBiO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWaErhOU1Vuw"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/avsshw/AVSS.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AVSS"
      ],
      "metadata": {
        "id": "Ua0RqhLO1grP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "7_PSL6Rc1it3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We trained our model on the dataset of the following format:\n",
        "\n",
        "\n",
        "```bash\n",
        "NameOfTheDirectoryWithUtterances/\n",
        "├── audio/\n",
        "│   ├── train/\n",
        "│   │   ├── mix/          # Mixed audio utterances (2-speaker mixtures)\n",
        "│   │   ├── s1/          \n",
        "│   │   └── s2/          \n",
        "│   └── val/\n",
        "│       ├── mix/\n",
        "│       ├── s1/\n",
        "│       └── s2/\n",
        "└── mouths/\n",
        "    ├── train/            \n",
        "    │   ├── SpeakerID1.npz\n",
        "    │   ├── SpeakerID2.npz\n",
        "    │   └── ...\n",
        "    └── val/\n",
        "        ├── SpeakerID1.npz\n",
        "        ├── SpeakerID2.npz\n",
        "        └── ...\n",
        "```"
      ],
      "metadata": {
        "id": "kmFKLUGXXZx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be evaluated with the following commands which you can follow if needed:\n",
        "\n",
        "1.\n",
        "```bash\n",
        "gdown https://drive.google.com/uc?id=1t7FFsG3hPcgUYuitekSMpggYLvzV6SXW\n",
        "unzip /content/AVSS-main/rtfs.zip\n",
        "```\n",
        "\n",
        "2. Assuming you have dla_dataset directory in the root of the project (or you can change it via hydra option datasets.test.data_dir=your/dataset):\n",
        "```bash\n",
        "python3 inference.py inferencer.from_pretrained=rtfs_improved/model_best.pth datasets=val_inference\n",
        "```"
      ],
      "metadata": {
        "id": "7w_1MXgGabBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "!uv run python3 inference.py inferencer.from_pretrained=rtfs_improved/model_best.pth datasets=val_inference"
      ],
      "metadata": {
        "id": "-R6kcFpxxWay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom inference"
      ],
      "metadata": {
        "id": "S4UTBE0rbr1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you wish to run inference on your custom dataset, our model expects data in the following structure:\n",
        "\n",
        "\n",
        "```bash\n",
        "NameOfTheDirectoryWithUtterances\n",
        "├── audio\n",
        "│   ├── mix\n",
        "│   │   ├── FirstSpeakerID1_SecondSpeakerID1.wav # also may be flac or mp3\n",
        "│   │   ├── FirstSpeakerID2_SecondSpeakerID2.wav\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   └── FirstSpeakerIDn_SecondSpeakerIDn.wav\n",
        "│   ├── s1 # ground truth for the speaker s1, may not be given\n",
        "│   │   ├── FirstSpeakerID1_SecondSpeakerID1.wav # also may be flac or mp3\n",
        "│   │   ├── FirstSpeakerID2_SecondSpeakerID2.wav\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   .\n",
        "│   │   └── FirstSpeakerIDn_SecondSpeakerIDn.wav\n",
        "│   └── s2 # ground truth for the speaker s2, may not be given\n",
        "│       ├── FirstSpeakerID1_SecondSpeakerID1.wav # also may be flac or mp3\n",
        "│       ├── FirstSpeakerID2_SecondSpeakerID2.wav\n",
        "│       .\n",
        "│       .\n",
        "│       .\n",
        "│       └── FirstSpeakerIDn_SecondSpeakerIDn.wav\n",
        "└── mouths # contains video information for all speakers\n",
        "    ├── FirstOrSecondSpeakerID1.npz # npz mouth-crop\n",
        "    ├── FirstOrSecondSpeakerID2.npz\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "    └── FirstOrSecondSpeakerIDn.npz\n",
        "```"
      ],
      "metadata": {
        "id": "WtmD-ubJ13ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We provide a small example dataset with ground truths. To run inference:**"
      ],
      "metadata": {
        "id": "WnBlDTEcbIuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Download the data (you can pass the link to your YandexDisk dataset in the .zip format here):"
      ],
      "metadata": {
        "id": "FtauQ46BbMmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv run python3 scripts/download_inference_data.py --link https://disk.yandex.ru/d/h2t8ItWMdne2ZA --download_location ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwePmLjI3KFV",
        "outputId": "525f8bf6-0411-4e2f-ae09-964a286be9f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n",
            "Extracting ./inference_dataset.zip...\n",
            "Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our example dataset you can also use\n",
        "```bash\n",
        "sh scripts/inference.sh\n",
        "```\n",
        "\n",
        "but we provide you with a full comand for your own use above."
      ],
      "metadata": {
        "id": "KfCgo9pMj3FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Download the pretrained model:"
      ],
      "metadata": {
        "id": "Z4IhED4Piooi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1l72LuBr_CQxaut6WUbyyPFJIRIbH8-68\n",
        "!unzip /content/AVSS-main/rtfs.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtXExN3c4T2a",
        "outputId": "b7372753-1cc6-46cd-a799-e385b6deb0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1t7FFsG3hPcgUYuitekSMpggYLvzV6SXW\n",
            "From (redirected): https://drive.google.com/uc?id=1t7FFsG3hPcgUYuitekSMpggYLvzV6SXW&confirm=t&uuid=0b4a367a-a576-459f-88ea-c43522f67ab7\n",
            "To: /content/AVSS-main/rtfs.zip\n",
            "100% 311M/311M [00:02<00:00, 125MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Set environment & run inference:\n",
        "\n",
        "(Predictions will be saved to data/saved/inference_custom_dir/test, works even if you don't have ground truth like we do)"
      ],
      "metadata": {
        "id": "-vmWUYpqitMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "!sh scripts/inference.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSyxCZgA118h",
        "outputId": "4054926e-3f4b-42bd-da26-c081d939c1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RTFSNet(\n",
            "  (encoder): STFT()\n",
            "  (decoder): ISTFT()\n",
            "  (blocks): ModuleList(\n",
            "    (0): RTFSBlock(\n",
            "      (input_proj): Linear(in_features=2, out_features=64, bias=True)\n",
            "      (freq_rnn): FrequencyRNN(\n",
            "        (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (time_rnn): TimeRNN(\n",
            "        (rnn): LSTM(128, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (tf_interaction): TFInteraction(\n",
            "        (freq_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (time_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "        )\n",
            "        (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (output_proj): Linear(in_features=128, out_features=64, bias=True)\n",
            "    )\n",
            "    (1-3): 3 x RTFSBlock(\n",
            "      (input_proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (freq_rnn): FrequencyRNN(\n",
            "        (rnn): LSTM(64, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (time_rnn): TimeRNN(\n",
            "        (rnn): LSTM(128, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (tf_interaction): TFInteraction(\n",
            "        (freq_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (time_conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128)\n",
            "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (ffn): Sequential(\n",
            "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "        )\n",
            "        (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (output_proj): Linear(in_features=128, out_features=64, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (mask_estimator): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=4, bias=True)\n",
            "    (3): Tanh()\n",
            "  )\n",
            ")\n",
            "Loading model weights from: rtfs/model_best.pth ...\n",
            "test: 100% 3/3 [00:03<00:00,  1.25s/it]\n",
            "    test_SI-SNRi   : 12.02201239267985\n",
            "    test_SDRi      : 12.440301577250162\n",
            "    test_PESQ      : 2.0738654931386313\n",
            "    test_STOI      : 0.8945748607317606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluate metrics (Optional)"
      ],
      "metadata": {
        "id": "l84ZXI5_cPbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your dataset includes ground-truth clean sources (s1/, s2/), you can compute metrics separately:"
      ],
      "metadata": {
        "id": "4nU0LUY8gPFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv run python3 calc_metrics.py \\\n",
        "--predictions_dir data/saved/inference_custom_dir/test \\\n",
        "--ground_truth_dir inference_dataset/audio \\\n",
        "--mixture_dir inference_dataset/audio/mix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kebOdzE-f38K",
        "outputId": "a0c13fda-ea11-4202-c2ab-8044437abfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 3/3 [00:02<00:00,  1.30it/s]\n",
            "SI-SNRi        : 12.0220\n",
            "SDRi           : 12.4403\n",
            "PESQ           : 2.0737\n",
            "STOI           : 0.8946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script assumes:\n",
        "\n",
        "Predictions are .wav files with the same names as mixtures.\n",
        "\n",
        "Ground truth is split into s1/ and s2/ subdirectories.\n",
        "\n",
        "Mixtures are in mixture_dir."
      ],
      "metadata": {
        "id": "9ExptSgTi_Bf"
      }
    }
  ]
}
